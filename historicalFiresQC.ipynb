{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "historicalFiresQC.ipynb",
      "provenance": [],
      "mount_file_id": "1R7lPQj3tBjTsyoi1Hm3Dg5NwnXtIgHyY",
      "authorship_tag": "ABX9TyM1bWQKnoWTS+MXuJWt8Bb0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/boothmanrylan/historicalFiresQC/blob/refdata/historicalFiresQC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoy9aEvv9rik"
      },
      "source": [
        "# Clone Repo and Load Modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqTaksZydt5L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1213ef6d-eaec-429d-8af2-a50deb1231c5"
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "from google.colab import drive\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "drive.mount('/content/drive/')\n",
        "%cd /content/drive/My\\ Drive/historicalFiresQC\n",
        "\n",
        "# if the directory doesn't exist clone it from github\n",
        "! if [ ! -d ./historicalFiresQC/ ]; then git clone https://github.com/boothmanrylan/historicalFiresQC.git; fi\n",
        "\n",
        "\n",
        "%cd ./historicalFiresQC/\n",
        "! git pull\n",
        "! git checkout refdata\n",
        "%cd ../\n",
        "\n",
        "! pip install q -r ./historicalFiresQC/requirements.txt\n",
        "\n",
        "# import modules from historicalFiresQC\n",
        "from historicalFiresQC import model as Model\n",
        "from historicalFiresQC import data as Data\n",
        "from historicalFiresQC import assessment as Assessment\n",
        "from historicalFiresQC import visualize as Visualize"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n",
            "/content/drive/My Drive/historicalFiresQC\n",
            "/content/drive/My Drive/historicalFiresQC/historicalFiresQC\n",
            "Already up to date.\n",
            "Already on 'refdata'\n",
            "Your branch is up to date with 'origin/refdata'.\n",
            "/content/drive/My Drive/historicalFiresQC\n",
            "Collecting git+https://github.com/tensorflow/examples.git (from -r ./historicalFiresQC/requirements.txt (line 1))\n",
            "  Cloning https://github.com/tensorflow/examples.git to /tmp/pip-req-build-8o3ybtbm\n",
            "  Running command git clone -q https://github.com/tensorflow/examples.git /tmp/pip-req-build-8o3ybtbm\n",
            "Requirement already satisfied (use --upgrade to upgrade): tensorflow-examples===0e7a63baa06c0afd133a98eee95457533b7f9dc1- from git+https://github.com/tensorflow/examples.git in /usr/local/lib/python3.6/dist-packages (from -r ./historicalFiresQC/requirements.txt (line 1))\n",
            "Requirement already satisfied: q in /usr/local/lib/python3.6/dist-packages (2.6)\n",
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.6/dist-packages (from -r ./historicalFiresQC/requirements.txt (line 2)) (0.8.3)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from tensorflow-examples===0e7a63baa06c0afd133a98eee95457533b7f9dc1-->-r ./historicalFiresQC/requirements.txt (line 1)) (0.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorflow-examples===0e7a63baa06c0afd133a98eee95457533b7f9dc1-->-r ./historicalFiresQC/requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: typeguard in /usr/local/lib/python3.6/dist-packages (from tensorflow-addons->-r ./historicalFiresQC/requirements.txt (line 2)) (2.7.1)\n",
            "Building wheels for collected packages: tensorflow-examples\n",
            "  Building wheel for tensorflow-examples (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tensorflow-examples: filename=tensorflow_examples-0e7a63baa06c0afd133a98eee95457533b7f9dc1_-cp36-none-any.whl size=157025 sha256=0576c5e77eb9f6489f36100f32ee13493e938e5610b6456f5597e7ed7bea07ea\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ydsw92xr/wheels/83/64/b3/4cfa02dc6f9d16bf7257892c6a7ec602cd7e0ff6ec4d7d714d\n",
            "Successfully built tensorflow-examples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiHJasLK97f2"
      },
      "source": [
        "# Reload Modules\n",
        "\n",
        "This is only necessary to run if changes have been made to the historicalFiresQC modules since you started using the colab notebook. It will pull the changes and reload the modules, allowing the changes to be incorporated without having to restart the runtime. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRMz5uBq3IT7",
        "outputId": "24baeb51-c67d-47d2-f6fe-7c27e9ae12e9"
      },
      "source": [
        "%cd ./historicalFiresQC\n",
        "! git pull\n",
        "%cd ../\n",
        "\n",
        "from importlib import reload\n",
        "\n",
        "Model = reload(Model)\n",
        "Data = reload(Data)\n",
        "Assessment = reload(Assessment)\n",
        "Visualize = reload(Assessment)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/historicalFiresQC/historicalFiresQC\n",
            "remote: Enumerating objects: 5, done.\u001b[K\n",
            "remote: Counting objects: 100% (5/5), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1/1), done.\u001b[K\n",
            "remote: Total 3 (delta 2), reused 3 (delta 2), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (3/3), done.\n",
            "From https://github.com/boothmanrylan/historicalFiresQC\n",
            "   4c10156..4a98cce  refdata    -> origin/refdata\n",
            "Updating 4c10156..4a98cce\n",
            "Fast-forward\n",
            " assessment.py | 2 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
            " 1 file changed, 1 insertion(+), 1 deletion(-)\n",
            "/content/drive/My Drive/historicalFiresQC\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oopAhuNt-PCO"
      },
      "source": [
        "# Global Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ThyYOwggo84"
      },
      "source": [
        "\n",
        "PATH = '/content/drive/My Drive/historicalFiresQC/'\n",
        "DATAPATH = PATH + 'Data/Reference Annotations/'\n",
        "MODELPATH = PATH + 'Models/CleanAnnotations_CombinedBurntClasses/'\n",
        "SHAPE = (128, 128, 4)\n",
        "CLASSES = 5\n",
        "BATCH_SIZE = 64\n",
        "LEARNING_RATE = 1e-4\n",
        "EPOCHS = 10\n",
        "STEPS_PER_EPOCH = 10\n",
        "\n",
        "# setting this to true will load the model/weights from MODELPATH\n",
        "LOADMODEL = True\n",
        "\n",
        "# setting this to false will skip the model training step, therefore\n",
        "# only set to false if LOADMODEL is true\n",
        "TRAINMODEL = False\n",
        "\n",
        "# if LOADMODEL and TRAINMODEL are both true, the model/weights saved at\n",
        "# MODELPATH will be overwritten after the model is retrained\n",
        "\n",
        "\n",
        "# dataset options\n",
        "train_pattern = DATAPATH + 'train*.tfrecord.gz'\n",
        "val_pattern = DATAPATH + 'val*.tfrecord.gz'\n",
        "test_pattern = DATAPATH + 'test*.tfrecord.gz'\n",
        "clean_annotation = False\n",
        "noisy_annotation = False\n",
        "combined_burnt = False\n",
        "split_burnt = False\n",
        "get_ref_points = True\n",
        "get_merged_ref_points = False\n",
        "get_burn_age = False\n",
        "get_merged_burn_age = False"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drGmj9wd-dxK"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81TufT8Y-aDs",
        "outputId": "92c36afe-3edc-442c-f1b4-acd71f635ca1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print('Getting Training Data...')\n",
        "train_dataset = Data.get_dataset(\n",
        "    train_pattern, SHAPE[:-1], batch_size=BATCH_SIZE, filters=Data.filter_no_burnt,\n",
        "    shuffle=True, repeat=True, prefetch=True, clean_annotation=clean_annotation,\n",
        "    noisy_annotation=noisy_annotation, combined_burnt=combined_burnt,\n",
        "    split_burnt=split_burnt, get_ref_points=get_ref_points,\n",
        "    get_merged_ref_points=get_merged_ref_points, get_burn_age=get_burn_age,\n",
        "    get_merged_burn_age=get_merged_burn_age\n",
        ")\n",
        "print('Done Getting Training Data.\\n')\n",
        "\n",
        "print('Getting Validation Data...')\n",
        "val_dataset = Data.get_dataset(\n",
        "    val_pattern, SHAPE[:-1], batch_size=BATCH_SIZE, filters=None,\n",
        "    shuffle=False, repeat=False, prefetch=True, clean_annotation=clean_annotation,\n",
        "    noisy_annotation=noisy_annotation, combined_burnt=combined_burnt,\n",
        "    split_burnt=split_burnt, get_ref_points=get_ref_points,\n",
        "    get_merged_ref_points=get_merged_ref_points, get_burn_age=get_burn_age,\n",
        "    get_merged_burn_age=get_merged_burn_age\n",
        ")\n",
        "print('Done Getting Validation Data.\\n')\n",
        "\n",
        "print('Getting Test Data...')\n",
        "test_dataset = Data.get_dataset(\n",
        "    test_pattern, SHAPE[:-1], batch_size=1, filters=None,\n",
        "    shuffle=False, repeat=False, prefetch=True, clean_annotation=clean_annotation,\n",
        "    noisy_annotation=noisy_annotation, combined_burnt=combined_burnt,\n",
        "    split_burnt=split_burnt, get_ref_points=get_ref_points,\n",
        "    get_merged_ref_points=get_merged_ref_points, get_burn_age=get_burn_age,\n",
        "    get_merged_burn_age=get_merged_burn_age\n",
        ")\n",
        "print('Done Getting Test Data.\\n')\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Getting Training Data...\n",
            "Done Getting Training Data.\n",
            "\n",
            "Getting Validation Data...\n",
            "Done Getting Validation Data.\n",
            "\n",
            "Getting Test Data...\n",
            "Done Getting Test Data.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jDgWHrO-5Bq"
      },
      "source": [
        "# Build Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATCx0pqc-j7j",
        "outputId": "9a79bfa0-268b-4560-bf59-4f69b7b62ab0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print('Building Model...')\n",
        "model = Model.build_unet_model(input_shape=SHAPE, classes=CLASSES)\n",
        "if LOADMODEL:\n",
        "    if os.path.exists(MODELPATH + 'saved_model.pb'): # load the entire model\n",
        "        model = tf.keras.models.load_model(MODELPATH)\n",
        "    else: # just load the model weights\n",
        "        model.load_weights(MODELPATH)\n",
        "print('Done Building Model.\\n')\n",
        "\n",
        "if TRAINMODEL:\n",
        "    checkpoint = tf.keras.callbacks.ModelCheckPoint(\n",
        "        filepath=MODELPATH, save_weights_only=True,\n",
        "        save_freq=10 * STEPS_PER_EPOCH\n",
        "    )\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
        "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    metrics=['accuracy']\n",
        "    \n",
        "    print('Compiling Model...')\n",
        "    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
        "    print('Done Compiling Model.\\n')\n",
        "    \n",
        "    print('Training Model...')\n",
        "    model.fit(\n",
        "        train_dataset, epochs=EPOCHS, verbose=1,\n",
        "        steps_per_epoch=STEPS_PER_EPOCH, callback=[checkpoint]\n",
        "    )\n",
        "    print('Done Traing Model.\\n')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building Model...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/applications/imagenet_utils.py:333: UserWarning: This model usually expects 1 or 3 input channels. However, it was passed an input_shape with 4 input channels.\n",
            "  str(input_shape[-1]) + ' input channels.')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Done Building Model.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMFS4ZIcM10d"
      },
      "source": [
        "# Model Assessment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KXLtkU2M-ZG"
      },
      "source": [
        "def plot_burn_age_accuracy(model, dataset, class_labels):\n",
        "    num_classes = len(class_labels)\n",
        "    results = Assessment.dated_burn_accuracy(model, dataset, num_classes)\n",
        "    df = pd.DataFrame.from_dict(results)\n",
        "    df.index = class_labels\n",
        "    df = df.drop(0, 1)\n",
        "    df /= df.sum()\n",
        "    df = df.melt(ignore_index=False).reset_index()\n",
        "    df.columns = ['Predicted Class', 'Burn Age', 'Percentage']\n",
        "    sns.catplot(x='Burn Age', hue='Predicted Class', y='Count', data=df,\n",
        "                kind='bar', height=8, aspect=2)\n",
        "\n",
        "def assessment(model, datasetpattern, merged_refs, class_labels):\n",
        "    reference_dataset = Data.get_dataset(\n",
        "        datasetpattern, SHAPE[:-1], shuffle=False, repeat=False,\n",
        "        clean_annotation=False, noisy_annotation=False, combined_burnt=False,\n",
        "        split_burnt=False, get_ref_points=(not merged_refs),\n",
        "        get_merged_ref_points=merged_refs, get_burn_age=False,\n",
        "        get_merged_burn_age=False\n",
        "    )\n",
        "\n",
        "    burn_age_dataset = Data.get_dataset(\n",
        "        datasetpattern, SHAPE[:-1], shuffle=False, repeat=False,\n",
        "        clean_annotation=False, noisy_annotation=False, combined_burnt=False,\n",
        "        split_burnt=False, get_ref_points=False, get_merged_ref_points=False,\n",
        "        get_burn_age=(not merged_refs), get_merged_burn_age=merged_refs\n",
        "    )\n",
        "\n",
        "    num_classes = len(class_labels)\n",
        "\n",
        "    confusion_matrix = Assessment.reference_accuracy(\n",
        "        model, reference_dataset, num_classes\n",
        "    )\n",
        "\n",
        "    norm_confusion_matrix = Assessment.normalize_confusion_matrix(\n",
        "        confusion_matrix\n",
        "    )\n",
        "\n",
        "    # Assessment.plot_confusion_matrix(\n",
        "    #     norm_confusion_matrix, class_labels\n",
        "    # )\n",
        "\n",
        "    # Assessment.plot_burn_accuracy_by_burn_age(\n",
        "        # model, burn_age_dataset, class_labels\n",
        "    # )\n",
        "\n",
        "    plot_burn_age_accuracy(model, burn_age_dataset, class_labels)\n",
        "\n",
        "\n",
        "class_labels = ['None', 'Cloud', 'Water', 'Land', 'Burn']\n",
        "assessment(model, [val_pattern, test_pattern], False, class_labels)\n",
        "# assessment(model, [val_pattern, test_pattern], True, class_labels)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}